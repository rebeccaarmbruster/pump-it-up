{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and viewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('./Data/training_set_values.csv')\n",
    "test_raw = pd.read_csv('./Data/test_set_values.csv')\n",
    "\n",
    "train_raw['train'] = 1\n",
    "test_raw['train'] = 0\n",
    "\n",
    "data = pd.concat([train_raw, test_raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '0.0'\n",
    "\n",
    "int_var = ['population','gps_height', 'construction_year']\n",
    "float_var = ['amount_tsh','longitude']\n",
    "\n",
    "features_to_drop = ['num_private','recorded_by']\n",
    "\n",
    "null_features = ['longitude','latitude','gps_height','population','construction_year','amount_tsh']\n",
    "\n",
    "divisions = ['region', 'ward']\n",
    "divisions_total = ['ward', 'region', 'overall']\n",
    "\n",
    "num_features = ['latitude','longitude','operation_years','amount_tsh', 'gps_height', 'population']\n",
    "\n",
    "cat_features = 'funder installer wpt_name basin subvillage region lga ward public_meeting scheme_management scheme_name permit extraction_type extraction_type_group extraction_type_class management management_group payment payment_type water_quality quality_group quantity quantity_group source source_type source_class waterpoint_type waterpoint_type_group'.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify missing values in numerical data\n",
    "\n",
    "for var in int_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0]))\n",
    "\n",
    "for var in float_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0.0]))\n",
    "\n",
    "print('latitude:')\n",
    "display(data['latitude'].min())\n",
    "display(len(data[(data['latitude'] > -0.001) & (data['latitude'] < 0.001)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros by NaN\n",
    "\n",
    "for var in int_var:\n",
    "    data[var].replace(0, np.nan, inplace=True)\n",
    "    \n",
    "for var in float_var:\n",
    "    data[var].replace(0.0, np.nan, inplace=True)\n",
    "\n",
    "data['latitude'].where((data['latitude'] < -0.001) | (data['latitude'] > 0.001), other= np.nan, inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic scaling of amount_tsh and population\n",
    "\n",
    "data['amount_tsh']=data.apply(lambda row: np.log1p(row['amount_tsh']),axis=1)\n",
    "data['population']=data.apply(lambda row: np.log1p(row['population']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_recorded' from datetime to year only\n",
    "data['date_recorded'] = pd.to_datetime(data['date_recorded'])\n",
    "data['date_recorded'] = data['date_recorded'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "train = data[data['train'] == 1]\n",
    "test = data[data['train'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values in numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate critical columnns for imputation based on normal distribution and random choice\n",
    "\n",
    "for null_feature in null_features:\n",
    "    data['_'.join([null_feature, 'imp_normal'])] = data[null_feature]\n",
    "    data['_'.join([null_feature, 'imp_random_choice'])] = data[null_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for mean and standard deviation of critical features based on 'region', 'ward' and 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    data['_'.join([null_feature, 'mean', 'overall'])] = train[null_feature].mean()\n",
    "    data['_'.join([null_feature, 'std', 'overall'])] = train[null_feature].std()\n",
    "    for division in divisions:\n",
    "        new_feature_name_mean = '_'.join([null_feature, 'mean', division])\n",
    "        new_feature_name_std = '_'.join([null_feature, 'std', division])\n",
    "        \n",
    "        calcs_mean = train.groupby(division)[null_feature].mean()\n",
    "        calcs_std = train.groupby(division)[null_feature].std()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_mean] = calcs_mean[value]\n",
    "            data.loc[data[division]==value, new_feature_name_std] = calcs_std[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with random numbers generated by normal distribution based on mean, std by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        data['_'.join([null_feature,'imp_normal'])] = data.apply(lambda row: np.random.normal(loc=row['_'.join([null_feature,'mean',division])], scale=row['_'.join([null_feature,'std',division])]) if math.isnan(row['_'.join([null_feature,'imp_normal'])]) else row['_'.join([null_feature,'imp_normal'])], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_normal'])].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add columns with list of values in corresponding group of 'region' and 'ward', respectively\n",
    "\n",
    "for null_feature in null_features:\n",
    "    overall_list = list(train[null_feature])\n",
    "    overall_list = [x for x in overall_list if not math.isnan(x)]\n",
    "    data['_'.join([null_feature, 'list', 'overall'])] = data.apply(lambda row: overall_list, axis=1)\n",
    "    display(null_feature, 'overall list done')\n",
    "    for division in divisions:\n",
    "        feature_name = '_'.join([null_feature, 'list', division])\n",
    "        lists = train.groupby(division)[null_feature].apply(list)\n",
    "        data[feature_name] = data.apply(lambda row: list() if row[division] not in train[division].unique() else lists[row[division]], axis=1)\n",
    "        data[feature_name] = data[feature_name].apply(lambda lst: [x for x in lst if not math.isnan(x)])\n",
    "        data[feature_name] = data[feature_name].apply(lambda x: np.nan if not x else x)\n",
    "        display('List for {} by {} created'.format(null_feature, division))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with empirical distribution grouped by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:        \n",
    "        #data['_'.join([null_feature,'imp_random_choice'])] = data.apply(lambda row: np.random.choice(a=row['_'.join([null_feature,'list',division])]) if math.isnan(row['_'.join([null_feature,'imp_random_choice'])]) else row['_'.join([null_feature,'imp_random_choice'])], axis=1)\n",
    "        data['_'.join([null_feature,'imp_random_choice'])] = data.apply(lambda row: row['_'.join([null_feature,'imp_random_choice'])] if not np.isnan(row['_'.join([null_feature,'imp_random_choice'])]).any() else (np.random.choice(a=row['_'.join([null_feature,'list',division])]) if not np.isnan(row['_'.join([null_feature,'list',division])]).any() else np.nan), axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_random_choice'])].isnull().sum()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for median of critical integer features based on 'region', 'ward', 'overall'\n",
    "\n",
    "float_var.append('latitude')\n",
    "\n",
    "for var in int_var:\n",
    "    data['_'.join([var, 'median', 'overall'])] = train[var].median()\n",
    "    for division in divisions:\n",
    "        new_feature_name_median = '_'.join([var, 'median', division])\n",
    "        calcs_median = train.groupby(division)[var].median()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_median] = calcs_median[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with mean and median by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with mean and median by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with overall mean and median\n",
    "\n",
    "for var in float_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'mean',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))\n",
    "\n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'median',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of train data (for problem-based preprocessing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "train = data[data['train'] == 1]\n",
    "test = data[data['train'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of unique values in each categorical feature, if this number of unique values is larger than 125\n",
    "# Show number of last 30% of unique values and number of levels with a frequency of only one\n",
    "\n",
    "categorical_features = 'funder installer wpt_name basin subvillage region lga ward recorded_by scheme_management scheme_name extraction_type extraction_type_group extraction_type_class management management_group payment payment_type water_quality quality_group quantity quantity_group source source_type source_class waterpoint_type waterpoint_type_group'.split()\n",
    "features_to_group = dict()\n",
    "for feature in categorical_features:\n",
    "    num = train[feature].nunique()\n",
    "    if num > 125:\n",
    "        last_30_pct = int(num*0.3)\n",
    "        print('{}: {}, 30%: {}'.format(feature, num, last_30_pct))\n",
    "        freq_one_counter = 0\n",
    "        groups = train.groupby(feature).size()\n",
    "        for i in range(len(groups)):\n",
    "            if groups[i] == 1:\n",
    "                freq_one_counter += 1\n",
    "        features_to_group[feature] = [last_30_pct, freq_one_counter]\n",
    "        print('   Levels with frequency of one: {}'.format(freq_one_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by aggregating by last 30% or levels with frequency of one - depending on which value of grouped levels is higher\n",
    "\n",
    "for feature, values in features_to_group.items():\n",
    "    values_30_pct = values[0]\n",
    "    values_freq_one = values[1]\n",
    "    total_levels = train[feature].nunique()\n",
    "    print('{} - Total levels: {}; 30%: {}, levels with frequency of one: {}'.format(feature, total_levels, values_30_pct, values_freq_one))\n",
    "    num_to_replace = values_freq_one if values_freq_one > values_30_pct else values_30_pct\n",
    "    least_common_values = list(train[feature].value_counts().index[-num_to_replace:])\n",
    "    train[feature] = train.apply(lambda row: 'Others' if row[feature] in least_common_values else row[feature], axis=1)\n",
    "    print('{} - Number of grouped levels: {}, Total levels after grouping: {}'.format(feature, num_to_replace, train[feature].nunique()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing of train data (for problem-based preprocessing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "train = data[data['train'] == 1]\n",
    "test = data[data['train'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to train data set\n",
    "labels = pd.read_csv('./Data/training_set_labels.csv')\n",
    "train = train.merge(labels, on=\"id\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of target class instances\n",
    "train.groupby('status_group').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes for each underrepresented target class\n",
    "needs_repair_df = train.loc[train['status_group'] == 'functional needs repair']\n",
    "non_functional_df = train.loc[train['status_group'] == 'non functional']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional 'non functional' rows to train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random rows for equal distribution of ‘non functional' class \n",
    "missing_non_functional = len(train.loc[train['status_group'] == 'functional']) - len(non_functional_df)\n",
    "additional_non_functionals = non_functional_df.sample(n=missing_non_functional, axis=0)\n",
    "train = pd.concat([train, additional_non_functionals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of target class instances after balancing 'non functional' target class\n",
    "train.groupby('status_group').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional 'needs repair' rows to train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional full sets of 'needs repair' target class\n",
    "missing_need_repairs = len(train.loc[train['status_group'] == 'functional']) - len(needs_repair_df)\n",
    "for i in range(missing_need_repairs // len(needs_repair_df)):\n",
    "    train = pd.concat([train, needs_repair_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of target class instances after first step of balancing 'needs repair' target class\n",
    "train.groupby('status_group').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random rows for equal distribution of ‘needs repair' class \n",
    "additional_needs_repairs = needs_repair_df.sample(n=missing_need_repairs % len(needs_repair_df), axis=0)\n",
    "train = pd.concat([train, additional_needs_repairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of target class instances after first step of balancing 'needs repair' target class\n",
    "train.groupby('status_group').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature that gives information about operational time\n",
    "\n",
    "imputation_methods = ['normal', 'random_choice']\n",
    "\n",
    "data['operation_years'] = data['date_recorded'] - data['construction_year']\n",
    "\n",
    "for method in imputation_methods:\n",
    "    data['_'.join(['operation_years_imp', method])] = data['date_recorded'].dt.year - data['_'.join(['construction_year_imp', method])]\n",
    "    data['_'.join(['operation_years_imp', method])] = data['_'.join(['operation_years_imp', method])].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns used for imputation and generation of random numbers\n",
    "\n",
    "drop_columns = list()\n",
    "measures = 'mean std list'.split()\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([null_feature, measure, division]))\n",
    "            \n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([var, 'median', division]))\n",
    "            \n",
    "data.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant features and features that do not seem to have an impact (initial feature selection)\n",
    "\n",
    "data.drop(columns=features_to_drop,axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace unique values that only appear in the test data set \n",
    "Either by 'Others' or by the most frequent value in the corresponding column of the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in cat_features:\n",
    "    display('{}, {}: {}'.format(cat, 'train', train[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'test', test[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'data', data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of features that contain values in the test which don't appear in the training data\n",
    "\n",
    "unique_test_features = list()\n",
    "for cat in cat_features:\n",
    "    if train[cat].nunique() < data[cat].nunique():\n",
    "        unique_test_features.append(cat)\n",
    "unique_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unique values that appear in the test dataset only to 'Others' if 'Others' appears in the training dataset, else set it to most frequent value in the corresponding column\n",
    "\n",
    "for feature in unique_test_features:\n",
    "    train_values = train[feature].unique().tolist()\n",
    "    test_values = test[feature].unique().tolist()\n",
    "    merged_values = pd.DataFrame(train_values).merge(pd.DataFrame(test_values), how='right', indicator=True)\n",
    "    unique_test_values = list(merged_values[0].loc[merged_values['_merge'] == 'right_only'])\n",
    "    replace_value = 'Others' if 'Others' in train_values else train[feature].mode()\n",
    "    for value in unique_test_values:\n",
    "        data.loc[data[feature] == value, feature] = replace_value    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in cat_features:\n",
    "    display('{}, {}: {}'.format(cat, 'NaNs', data[cat].isnull().sum()))\n",
    "    display('{}, {}: {}'.format(cat, 'train', train[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'test', test[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'overall', data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace missing values by 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in selected_cat_features:\n",
    "    data[feature].replace(np.nan, 'Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factorize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in selected_cat_features:\n",
    "    data[feature] = pd.factorize(data[feature])[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data[\"train\"] == 1]\n",
    "test_df = data[data[\"train\"] == 0]\n",
    "\n",
    "train_df.drop([\"train\"], axis=1, inplace=True)\n",
    "test_df.drop([\"train\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_df).to_csv(\"./Data/train_cleaned_v\" + data_version + \".csv\", index=False)\n",
    "pd.DataFrame(test_df).to_csv(\"./Data/test_cleaned_v\" + data_version + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
