{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import category_encoders as ce\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and viewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('./Data/training_set_values.csv')\n",
    "test_raw = pd.read_csv('./Data/test_set_values.csv')\n",
    "\n",
    "train_raw['train'] = 1\n",
    "test_raw['train'] = 0\n",
    "data = pd.concat([train_raw, test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = '0.0'\n",
    "\n",
    "int_var = ['population','gps_height', 'construction_year']\n",
    "float_var = ['amount_tsh','longitude']\n",
    "\n",
    "features_to_drop = ['num_private','recorded_by']\n",
    "\n",
    "null_features = ['longitude','latitude','gps_height','population','construction_year','amount_tsh']\n",
    "#no calculations for num_private since they are dropped later (too many missing values)\n",
    "\n",
    "divisions = ['region', 'ward']\n",
    "divisions_total = ['ward', 'region', 'overall']\n",
    "\n",
    "# These will be scaled\n",
    "num_features = ['latitude','longitude','operation_years','amount_tsh', 'gps_height', 'population']\n",
    "\n",
    "# These will be factorized\n",
    "cat_features = list(data.columns)\n",
    "for feature in num_features:\n",
    "    cat_features.remove(feature)\n",
    "for feature in features_to_drop:\n",
    "    cat_features.remove(feature)\n",
    "cat_features.remove('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify missing values in numerical data\n",
    "\n",
    "for var in int_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0]))\n",
    "\n",
    "for var in float_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0.0]))\n",
    "\n",
    "print('latitude:')\n",
    "display(data['latitude'].min())\n",
    "display(len(data[(data['latitude'] > -0.001) & (data['latitude'] < 0.001)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros by NaN\n",
    "\n",
    "for var in int_var:\n",
    "    data[var].replace(0, np.nan, inplace=True)\n",
    "    \n",
    "for var in float_var:\n",
    "    data[var].replace(0.0, np.nan, inplace=True)\n",
    "\n",
    "data['latitude'].where((data['latitude'] < -0.001) | (data['latitude'] > 0.001), other= np.nan, inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic scaling of amount_tsh and population\n",
    "\n",
    "data['amount_tsh']=data.apply(lambda row: np.log1p(row['amount_tsh']),axis=1)\n",
    "data['population']=data.apply(lambda row: np.log1p(row['population']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "train = data[data['train'] == 1]\n",
    "test = data[data['train'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values in numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate critical columnns for imputation based on normal distribution and random choice\n",
    "\n",
    "for null_feature in null_features:\n",
    "    #data['_'.join([null_feature, 'imp_mean-median'])] = data[null_feature]\n",
    "    data['_'.join([null_feature, 'imp_normal'])] = data[null_feature]\n",
    "    data['_'.join([null_feature, 'imp_random_choice'])] = data[null_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for mean and standard deviation of critical features based on 'region', 'ward' and 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    data['_'.join([null_feature, 'mean', 'overall'])] = train[null_feature].mean()\n",
    "    data['_'.join([null_feature, 'std', 'overall'])] = train[null_feature].std()\n",
    "    for division in divisions:\n",
    "        new_feature_name_mean = '_'.join([null_feature, 'mean', division])\n",
    "        new_feature_name_std = '_'.join([null_feature, 'std', division])\n",
    "        \n",
    "        calcs_mean = train.groupby(division)[null_feature].mean()\n",
    "        calcs_std = train.groupby(division)[null_feature].std()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_mean] = calcs_mean[value]\n",
    "            data.loc[data[division]==value, new_feature_name_std] = calcs_std[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with random numbers generated by normal distribution based on mean, std by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        data['_'.join([null_feature,'imp_normal'])] = data.apply(lambda row: np.random.normal(loc=row['_'.join([null_feature,'mean',division])], scale=row['_'.join([null_feature,'std',division])]) if math.isnan(row['_'.join([null_feature,'imp_normal'])]) else row['_'.join([null_feature,'imp_normal'])], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_normal'])].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add columns with list of values in corresponding group of 'region' and 'ward', respectively\n",
    "\n",
    "for null_feature in null_features:\n",
    "    overall_list = list(train[null_feature])\n",
    "    overall_list = [x for x in overall_list if not math.isnan(x)]\n",
    "    data['_'.join([null_feature, 'list', 'overall'])] = data.apply(lambda row: overall_list, axis=1)\n",
    "    display(null_feature, 'overall list done')\n",
    "    for division in divisions:\n",
    "        feature_name = '_'.join([null_feature, 'list', division])\n",
    "        lists = train.groupby(division)[null_feature].apply(list)\n",
    "        data[feature_name] = data.apply(lambda row: list() if row[division] not in train[division].unique() else lists[row[division]], axis=1)\n",
    "        data[feature_name] = data[feature_name].apply(lambda lst: [x for x in lst if not math.isnan(x)])\n",
    "        data[feature_name] = data[feature_name].apply(lambda x: np.nan if not x else x)\n",
    "        display('List for {} by {} created'.format(null_feature, division))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with empirical distribution grouped by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:        \n",
    "        #data['_'.join([null_feature,'imp_random_choice'])] = data.apply(lambda row: np.random.choice(a=row['_'.join([null_feature,'list',division])]) if math.isnan(row['_'.join([null_feature,'imp_random_choice'])]) else row['_'.join([null_feature,'imp_random_choice'])], axis=1)\n",
    "        data['_'.join([null_feature,'imp_random_choice'])] = data.apply(lambda row: row['_'.join([null_feature,'imp_random_choice'])] if not np.isnan(row['_'.join([null_feature,'imp_random_choice'])]).any() else (np.random.choice(a=row['_'.join([null_feature,'list',division])]) if not np.isnan(row['_'.join([null_feature,'list',division])]).any() else np.nan), axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_random_choice'])].isnull().sum()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of numerical features by mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for median of critical integer features based on 'region', 'ward', 'overall'\n",
    "\n",
    "float_var.append('latitude')\n",
    "\n",
    "for var in int_var:\n",
    "    data['_'.join([var, 'median', 'overall'])] = train[var].median()\n",
    "    for division in divisions:\n",
    "        new_feature_name_median = '_'.join([var, 'median', division])\n",
    "        calcs_median = train.groupby(division)[var].median()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_median] = calcs_median[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with mean and median by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with mean and median by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with overall mean and median\n",
    "\n",
    "for var in float_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'mean',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))\n",
    "\n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'median',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load imputed data from previous data files and add mean/median imputation as additional columns\n",
    "Can be used if only categorical features will be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imputed_train = pd.read_csv('./Data/train_cleaned_v0.1.csv')\n",
    "imputed_test = pd.read_csv('./Data/test_cleaned_v0.1.csv')\n",
    "\n",
    "imputed_data = pd.concat([imputed_train, imputed_test])'''\n",
    "#imputed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''keep_columns = 'id longitude latitude gps_height amount_tsh population construction_year'.split()\n",
    "for col in 'longitude latitude gps_height amount_tsh population construction_year'.split():\n",
    "    keep_columns.append('_'.join([col, 'imp_normal']))\n",
    "    keep_columns.append('_'.join([col, 'imp_random_choice']))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed_data = imputed_data[keep_columns]\n",
    "#imputed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''columns = imputed_data.columns\n",
    "new_columns = list()\n",
    "for col in columns:\n",
    "    if 'imp' in col or col == 'id':\n",
    "        new_columns.append(col)\n",
    "    else:\n",
    "        new_columns.append('_'.join([col, 'imp_mean-median']))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imputed_data.columns = new_columns\n",
    "imputed_data.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.merge(imputed_data, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for feature in null_features:\n",
    "    data[feature] = data['_'.join([feature, 'imp', 'mean-median'])]\n",
    "    data.drop('_'.join([feature, 'imp', 'mean-median']), inplace=True, axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature that gives information about operational time\n",
    "\n",
    "imputation_methods = ['normal', 'random_choice']\n",
    "data['date_recorded'] = pd.to_datetime(data['date_recorded'])\n",
    "\n",
    "data['operation_years'] = data['date_recorded'].dt.year - data['construction_year']\n",
    "\n",
    "for method in imputation_methods:\n",
    "    data['_'.join(['operation_years_imp', method])] = data['date_recorded'].dt.year - data['_'.join(['construction_year_imp', method])]\n",
    "    data['_'.join(['operation_years_imp', method])] = data['_'.join(['operation_years_imp', method])].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for s in split:\n",
    "    s[num_features] = scaler.fit_transform(s[num_features])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns used for imputation and generation of random numbers\n",
    "\n",
    "drop_columns = list()\n",
    "measures = 'mean std list'.split()\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([null_feature, measure, division]))\n",
    "            \n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([var, 'median', division]))\n",
    "            \n",
    "data.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant features and features that do not seem to have an impact\n",
    "\n",
    "data.drop(columns=features_to_drop,axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of categorical features\n",
    "\n",
    "No grouping and factorizing of categorical features in data version 0.1; missing values replaced by 'Missing' only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace unique values that only appear in the test data set \n",
    "Either by 'Others' or by the most frequent value in the corresponding column of the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in cat_features:\n",
    "    display('{}, {}: {}'.format(cat, 'train', train[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'test', test[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'data', data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of features that contain values in the test which don't appear in the training data\n",
    "\n",
    "unique_test_features = list()\n",
    "for cat in cat_features:\n",
    "    if train[cat].nunique() < data[cat].nunique():\n",
    "        unique_test_features.append(cat)\n",
    "unique_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unique values that appear in the test dataset only to 'Others' if 'Others' appears in the training dataset, else set it to most frequent value in the corresponding column\n",
    "\n",
    "for feature in unique_test_features:\n",
    "    train_values = train[feature].unique().tolist()\n",
    "    test_values = test[feature].unique().tolist()\n",
    "    merged_values = pd.DataFrame(train_values).merge(pd.DataFrame(test_values), how='right', indicator=True)\n",
    "    unique_test_values = list(merged_values[0].loc[merged_values['_merge'] == 'right_only'])\n",
    "    replace_value = 'Others' if 'Others' in train_values else train[feature].mode()\n",
    "    for value in unique_test_values:\n",
    "        data.loc[data[feature] == value, feature] = replace_value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in cat_features:\n",
    "    display('{}, {}: {}'.format(cat, 'NaNs', data[cat].isnull().sum()))\n",
    "    display('{}, {}: {}'.format(cat, 'train', train[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'test', test[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'overall', data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group feature categories other than top categories into 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension of categorical variables with high dimensionality \n",
    "\n",
    "'''dim_red_features = 'funder installer scheme_name lga ward'.split()\n",
    "for feature in dim_red_features:\n",
    "    train = train.assign(count = train.groupby(feature)[feature].transform('count')).sort_values(by = ['count',feature], ascending = [False,True])\n",
    "    top_values = train.drop_duplicates('count')\n",
    "    top_values = list(top_values.nlargest(10, 'count')[feature])\n",
    "    data[feature] = data[feature].apply(lambda x: x if (x in top_values) | (str(x) == 'nan') else 'Others')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for cat in cat_features:\n",
    "    display('{}, {}: {}'.format(cat, 'NaNs', data[cat].isnull().sum()))\n",
    "    display('{}, {}: {}'.format(cat, 'train', train[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'test', test[cat].nunique()))\n",
    "    display('{}, {}: {}'.format(cat, 'overall', data[cat].nunique()))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace missing values by 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in categorical features by 'Missing'\n",
    "\n",
    "for feature in cat_features:\n",
    "    data[feature].replace(np.nan, 'Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''data['funder'].value_counts()\n",
    "data['installer'].value_counts()\n",
    "data['scheme_name'].value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical features into numerical features by adding a column with their probability for each target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_labels = pd.read_csv('./Data/training_set_labels.csv')\n",
    "train = train.merge(train_labels, on=\"id\")\n",
    "train.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''for feature in cat_features:\n",
    "    train['count'] = train.groupby(feature)[feature].transform('count')\n",
    "train.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_percentage(groups, row, status_group, feature):\n",
    "    try:\n",
    "        sg_count = groups['count'].loc[(groups[feature] == row[feature]) & (groups['status_group'] == status_group)].item()\n",
    "        total_count = train['count'].loc[train['id'] == row['id']].item()\n",
    "        return sg_count / total_count\n",
    "    except ValueError:\n",
    "        return 0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''status_groups = 'functional,non functional,functional needs repair'.split(',')\n",
    "for feature in cat_features:\n",
    "    groups = pd.DataFrame({'count': train.groupby([feature, 'status_group']).size()}).reset_index()\n",
    "    for status_group in status_groups:\n",
    "        data['_'.join(['pct', feature, status_group])] = data.apply(lambda row: get_percentage(groups=groups, row=row, status_group=status_group, feature=feature), axis=1)\n",
    "    display(feature + ' done')'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factorize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize features for evaluations\n",
    "\n",
    "for feature in cat_features:\n",
    "    data[feature] = pd.factorize(data[feature])[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'subvillage' from cat_features its number of unique values leads to too many dummies\n",
    "#cat_features.remove('subvillage')\n",
    "# Get dummies for categorical features and add them to dataframe\n",
    "#data = pd.concat([data, pd.get_dummies(data[cat_features], dummy_na=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ce_bin = ce.BinaryEncoder(cols=cat_features)\n",
    "#data = ce_bin.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ce_hash = ce.HashingEncoder(cols=cat_features, n_components=len(cat_features * 20))\n",
    "#data = ce_hash.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data[\"train\"] == 1]\n",
    "test_df = data[data[\"train\"] == 0]\n",
    "\n",
    "train_df.drop([\"train\"], axis=1, inplace=True)\n",
    "test_df.drop([\"train\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_df).to_csv(\"./Data/train_cleaned_v\" + data_version + \".csv\", index=False)\n",
    "pd.DataFrame(test_df).to_csv(\"./Data/test_cleaned_v\" + data_version + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
