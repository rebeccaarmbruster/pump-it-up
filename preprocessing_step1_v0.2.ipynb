{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and viewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('./Data/training_set_values.csv')\n",
    "test_raw = pd.read_csv('./Data/test_set_values.csv')\n",
    "\n",
    "train_raw['train'] = 1\n",
    "test_raw['train'] = 0\n",
    "data = pd.concat([train_raw, test_raw])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify missing values in numerical data\n",
    "\n",
    "int_var = ['population','gps_height','construction_year']\n",
    "float_var = ['amount_tsh','longitude']\n",
    "\n",
    "for var in int_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0]))\n",
    "\n",
    "for var in float_var:\n",
    "    print('{}:'.format(var))\n",
    "    display(data[var].min())\n",
    "    display(len(data[data[var] == 0.0]))\n",
    "\n",
    "print('latitude:')\n",
    "display(data['latitude'].min())\n",
    "display(len(data[(data['latitude'] > -0.001) & (data['latitude'] < 0.001)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros by NaN\n",
    "\n",
    "for var in int_var:\n",
    "    data[var].replace(0, np.nan, inplace=True)\n",
    "    \n",
    "for var in float_var:\n",
    "    data[var].replace(0.0, np.nan, inplace=True)\n",
    "\n",
    "data['latitude'].where((data['latitude'] < -0.001) | (data['latitude'] > 0.001), other= np.nan, inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic scaling of amount_tsh and population (optional)\n",
    "\n",
    "data['amount_tsh']=data.apply(lambda row: np.log1p(row['amount_tsh']),axis=1)\n",
    "data['population']=data.apply(lambda row: np.log1p(row['population']),axis=1)\n",
    "\n",
    "train = data[data['train'] == 1]\n",
    "test = data[data['train'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate critical columnns for imputation based on normal distribution and random choice\n",
    "\n",
    "null_features = ['longitude','latitude','gps_height','population','construction_year','amount_tsh']\n",
    "#no calculations for num_private since they are dropped later (too many missing values)\n",
    "\n",
    "for null_feature in null_features:\n",
    "    data['_'.join([null_feature, 'imp_normal'])] = data[null_feature]\n",
    "    data['_'.join([null_feature, 'imp_random_choice'])] = data[null_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant features and features that do not seem to have an impact (choose variant)\n",
    "\n",
    "data.drop(['extraction_type_group','extraction_type_class','payment','quality_group','source_class','source_type','waterpoint_type_group','management_group','quantity_group','wpt_name','num_private','recorded_by'],axis=1,inplace=True)\n",
    "#data.drop(['num_private','recorded_by'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of numerical features by normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for mean and standard deviation of critical features based on 'region', 'ward' and 'overall'\n",
    "\n",
    "divisions = ['region', 'ward']\n",
    "\n",
    "for null_feature in null_features:\n",
    "    data['_'.join([null_feature, 'mean', 'overall'])] = train[null_feature].mean()\n",
    "    data['_'.join([null_feature, 'std', 'overall'])] = train[null_feature].std()\n",
    "    for division in divisions:\n",
    "        new_feature_name_mean = '_'.join([null_feature, 'mean', division])\n",
    "        new_feature_name_std = '_'.join([null_feature, 'std', division])\n",
    "        \n",
    "        calcs_mean = train.groupby(division)[null_feature].mean()\n",
    "        calcs_std = train.groupby(division)[null_feature].std()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_mean] = calcs_mean[value]\n",
    "            data.loc[data[division]==value, new_feature_name_std] = calcs_std[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with random numbers generated by normal distribution based on mean, std by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with random numbers generated by normal distribution based on mean, std by 'overall'\n",
    "\n",
    "divisions_total = ['ward', 'region', 'overall']\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        data['_'.join([null_feature,'imp_normal'])] = data.apply(lambda row: np.random.normal(loc=row['_'.join([null_feature,'mean',division])], scale=row['_'.join([null_feature,'std',division])]) if math.isnan(row['_'.join([null_feature,'imp_normal'])]) else row['_'.join([null_feature,'imp_normal'])], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_normal'])].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of numerical features by random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add columns with list of values in corresponding group of 'region' and 'ward', respectively\n",
    "\n",
    "for null_feature in null_features:\n",
    "    overall_list = list(train[null_feature])\n",
    "    overall_list = [x for x in overall_list if not math.isnan(x)]\n",
    "    data['_'.join([null_feature, 'list', 'overall'])] = data.apply(lambda row: overall_list, axis=1)\n",
    "    display(null_feature, 'overall list done')\n",
    "    for division in divisions:\n",
    "        feature_name = '_'.join([null_feature, 'list', division])\n",
    "        lists = train.groupby(division)[null_feature].apply(list)\n",
    "        data[feature_name] = data.apply(lambda row: list() if row[division] not in train[division].unique() else lists[row[division]], axis=1)\n",
    "        data[feature_name] = data[feature_name].apply(lambda lst: [x for x in lst if not math.isnan(x)])\n",
    "        data[feature_name] = data[feature_name].apply(lambda x: np.nan if not x else x)\n",
    "        display(null_feature, division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with empirical distribution grouped by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with empirical distribution grouped by 'overall'\n",
    "\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:        \n",
    "        data['_'.join([null_feature,'imp_random_choice'])] = data.apply(lambda row: row['_'.join([null_feature,'imp_random_choice'])] if not np.isnan(row['_'.join([null_feature,'imp_random_choice'])]).any() else (np.random.choice(a=row['_'.join([null_feature,'list',division])]) if not np.isnan(row['_'.join([null_feature,'list',division])]).any() else np.nan), axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(null_feature, division, data['_'.join([null_feature,'imp_random_choice'])].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of numerical features by mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for median of critical integer features based on region, ward, overall\n",
    "\n",
    "float_var.append('latitude')\n",
    "\n",
    "for var in int_var:\n",
    "    data['_'.join([var, 'median', 'overall'])] = train[var].median()\n",
    "    for division in divisions:\n",
    "        new_feature_name_median = '_'.join([var, 'median', division])\n",
    "        calcs_median = train.groupby(division)[var].median()\n",
    "        for value in train[division].unique() :\n",
    "            data.loc[data[division]==value, new_feature_name_median] = calcs_median[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: Impute missing values with mean and median by 'ward'\n",
    "# 2nd step (only applied on remaining null values): Impute missing values with mean and median by 'region'\n",
    "# 3rd step (only applied on remaining null values): Impute missing values with overall mean and median\n",
    "\n",
    "for var in float_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'mean',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))\n",
    "\n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        data[var] = data.apply(lambda row: row['_'.join([var,'median',division])] if math.isnan(row[var]) else row[var], axis=1)\n",
    "        display('Missing values after imputation in {} by {}: {}'.format(var, division, data[var].isnull().sum()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns used for imputation and generation of random numbers\n",
    "\n",
    "drop_columns = list()\n",
    "measures = 'mean std list'.split()\n",
    "for null_feature in null_features:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([null_feature, measure, division]))\n",
    "            \n",
    "for var in int_var:\n",
    "    for division in divisions_total:\n",
    "        for measure in measures:\n",
    "            drop_columns.append('_'.join([var, 'median', division]))\n",
    "            \n",
    "data.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature that gives information about operational time\n",
    "data['date_recorded'] = pd.to_datetime(data['date_recorded_x'])\n",
    "data['operation_years'] = data.date_recorded.dt.year - data.construction_year\n",
    "data['operation_years'] = data['operation_years'].astype(int)\n",
    "\n",
    "methods = ['imp_normal','imp_random_choice']\n",
    "for method in methods:\n",
    "    data['_'.join(['operation_years',method])] = data.date_recorded.dt.year - data['_'.join(['construction_year',method])]\n",
    "    data['_'.join(['operation_years',method])] = data['_'.join(['operation_years',method])].astype(int)\n",
    "data.drop(['date_recorded'],axis=1,inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of numerical features\n",
    "\n",
    "No scaling of numerical features for data version 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "'''\n",
    "num_features=['latitude','longitude','operation_years','amount_tsh', 'gps_height', 'population']\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for s in split:\n",
    "    s[num_features] = scaler.fit_transform(s[num_features])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of categorical features\n",
    "\n",
    "No grouping and factorizing of categorical features in data version 0.1; missing values replaced by 'Missing' only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in categorical features by 'Missing'\n",
    "\n",
    "cat_features = 'funder installer subvillage public_meeting scheme_management scheme_name permit'.split()\n",
    "\n",
    "for feature in cat_features:\n",
    "    data[feature].replace(np.nan, 'Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''data['funder'].value_counts()\n",
    "data['installer'].value_counts()\n",
    "data['scheme_name'].value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimension of categorical variables\n",
    "\n",
    "'''# 'funder'\n",
    "data = data.assign(count = data.groupby('funder')['funder'].transform('count'))\\\n",
    ".sort_values(by = ['count','funder'], ascending = [False,True])\n",
    "\n",
    "data.loc[data['count'] < 1050, 'funder'] = 'Others'\n",
    "data['funder'].replace(np.nan, 'Others', inplace=True)\n",
    "del data['count']\n",
    "\n",
    "# 'installer'\n",
    "data = data.assign(count = data.groupby('installer')['installer'].transform('count'))\\\n",
    ".sort_values(by = ['count','installer'], ascending = [False,True])\n",
    "\n",
    "data.loc[data['count'] < 765, 'installer'] = 'Others'\n",
    "data.loc[data['installer'] == '0', 'installer'] = 'Others'\n",
    "data['installer'].replace(np.nan, 'Others', inplace=True)\n",
    "del data['count']\n",
    "\n",
    "# 'scheme_name'\n",
    "data = data.assign(count = data.groupby('scheme_name')['scheme_name'].transform('count'))\\\n",
    ".sort_values(by = ['count','scheme_name'], ascending = [False,True])\n",
    "\n",
    "data.loc[data['count'] < 296, 'scheme_name'] = 'Others'\n",
    "data.loc[data['scheme_name'] == '0', 'scheme_name'] = 'Others'\n",
    "data['scheme_name'].replace(np.nan, 'Others', inplace=True)\n",
    "del data['count']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize features for evaluations\n",
    "\n",
    "data['funder'] = pd.factorize(data['funder'])[0]\n",
    "data['installer'] = pd.factorize(data['installer'])[0]\n",
    "data['basin'] = pd.factorize(data['basin'])[0]\n",
    "data['subvillage'] = pd.factorize(data['subvillage'])[0]\n",
    "data['region'] = pd.factorize(data['region'])[0]\n",
    "data['lga'] = pd.factorize(data['lga'])[0]\n",
    "data['ward'] = pd.factorize(data['ward'])[0]\n",
    "data['scheme_management'] = pd.factorize(data['scheme_management'])[0]\n",
    "data['scheme_name'] = pd.factorize(data['scheme_name'])[0]\n",
    "data['extraction_type'] = pd.factorize(data['extraction_type'])[0]\n",
    "data['management'] = pd.factorize(data['management'])[0]\n",
    "data['payment_type'] = pd.factorize(data['payment_type'])[0]\n",
    "data['water_quality'] = pd.factorize(data['water_quality'])[0]\n",
    "data['quantity'] = pd.factorize(data['quantity'])[0]\n",
    "data['waterpoint_type'] = pd.factorize(data['waterpoint_type'])[0]\n",
    "data['permit'] = pd.factorize(data['permit'])[0]\n",
    "data['source'] = pd.factorize(data['source'])[0]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data[\"train\"] == 1]\n",
    "test_df = data[data[\"train\"] == 0]\n",
    "\n",
    "train_df.drop([\"train\"], axis=1, inplace=True)\n",
    "test_df.drop([\"train\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_df).to_csv(\"./Data/train_cleaned_v0.2.csv\", index=False)\n",
    "pd.DataFrame(test_df).to_csv(\"./Data/test_cleaned_v0.2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
