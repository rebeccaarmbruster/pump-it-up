{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Data/train_cleaned_v0.2.csv')\n",
    "test = pd.read_csv('./Data/test_cleaned_v0.2.csv')\n",
    "\n",
    "labels = pd.read_csv('./Data/training_set_labels.csv')\n",
    "train = train.merge(labels, on=\"id\")\n",
    "target = train.pop(\"status_group\")\n",
    "\n",
    "train['train'] = 1\n",
    "test['train'] = 0\n",
    "\n",
    "data = pd.concat([train,test])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = ['amount_tsh','longitude','latitude','population','construction_year','gps_height','operation_years']\n",
    "\n",
    "# drop unnecessary features\n",
    "# drop 2 out of 3 from (mean/median, normal distribution, random choice)\n",
    "for feature in engineered_features:\n",
    "    #data[feature] = data['_'.join([feature,'imp_random_choice'])]\n",
    "    #data[feature] = data['_'.join([feature,'imp_normal'])]\n",
    "    data.drop(['_'.join([feature,'imp_normal'])], axis=1, inplace=True)\n",
    "    data.drop(['_'.join([feature,'imp_random_choice'])], axis=1, inplace=True)\n",
    "\n",
    "# optional: drop additional columns    \n",
    "# data.drop(['region_code','lga','district_code','scheme_name'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features (optional, not necessary for tree-based methods)\n",
    "\n",
    "#num_features=['latitude','longitude','operation_years', 'gps_height', 'population','amount_tsh','construction_year']\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#data[num_features] = scaler.fit_transform(data[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorize features for evaluations\n",
    "\n",
    "cat_features = ['funder','installer','basin','region','public_meeting','scheme_management','permit','extraction_type','management','payment_type','water_quality','payment_type','quantity','source','waterpoint_type','ward','subvillage','lga','scheme_name']\n",
    "\n",
    "for var in cat_features:\n",
    "    data[var].replace(np.nan, 'Missing', inplace=True)\n",
    "    data[var] = pd.factorize(data[var])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training/test sets\n",
    "\n",
    "train_df = data[data[\"train\"] == 1]\n",
    "test_df = data[data[\"train\"] == 0]\n",
    "train_df.drop([\"train\"], axis=1, inplace=True)\n",
    "train_df.drop(['id'],axis=1, inplace=True)\n",
    "test_df.drop([\"train\"], axis=1, inplace=True)\n",
    "\n",
    "id_test = test_df['id']\n",
    "test_df.drop(['id'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split (not necessary if desired output is submission file)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X = train\n",
    "#Y = target\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random grid to search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=20, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary if desired output is submission file\n",
    "\n",
    "#def evaluate(model, test_set, test_labels):\n",
    "#    predictions = model.predict(test_set)\n",
    "#    evaluation_df = pd.DataFrame(list())\n",
    "#    evaluation_df['true_values'] = list(test_labels)\n",
    "#    evaluation_df['predicted_values'] = list(predictions)\n",
    "#    correct_predictions = len(evaluation_df[evaluation_df['true_values'] == evaluation_df['predicted_values']])\n",
    "#    classification_rate = correct_predictions / len(predictions)\n",
    "    \n",
    "#    print('Model Performance')\n",
    "#    print('Accuracy = {:0.4f}%.'.format(classification_rate))\n",
    "    \n",
    "#    return classification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "best_random.fit(train_df, target)\n",
    "\n",
    "\n",
    "# following not possible if desired output is submission file\n",
    "\n",
    "#base_model = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "#base_model.fit(X_train, Y_train)\n",
    "#base_accuracy = evaluate(base_model, X_test, Y_test)\n",
    "\n",
    "#random_accuracy = evaluate(best_random, X_test, Y_test)\n",
    "\n",
    "#print('Improvement of {:0.4f}%.'.format(random_accuracy - base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_random.predict(test_df)\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions['id'] = id_test\n",
    "predictions.columns = ['status_group','id']\n",
    "predictions = predictions[['id','status_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into submission format\n",
    "\n",
    "formatsub = pd.read_csv('./Data/submission_format.csv')\n",
    "submission_format = pd.merge(formatsub, predictions, on=['id'], how='inner')\n",
    "submission_format.drop(['status_group_x'],axis=1,inplace=True)\n",
    "submission_format.columns = ['id','status_group']\n",
    "\n",
    "submission_format.to_csv('./Results/submission_format_rename.csv', index=False)\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Feature Importances (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(best_random.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list(train_df.columns), importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
